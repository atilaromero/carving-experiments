{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 - Train a model for each dataset category, comparing it to random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/atila.alr/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import models\n",
    "import report\n",
    "import block_sampler\n",
    "import batch_encoder\n",
    "from dataset import Dataset\n",
    "from report import Reporter2\n",
    "from trainer import Trainer, TrainResults\n",
    "from batch_encoder import Dataset, one_hot\n",
    "from block_sampler import count_sectors, BlockSamplerByFile, RandomSampler\n",
    "from batch_encoder import xs_encoder_8bits_11, BatchEncoder, xs_encoder_one_hot\n",
    "from filter_random import gen_rndchk_models, evaluate_rnd_model, filter_dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Activation, TimeDistributed, Flatten, Dot, Softmax, Lambda, RepeatVector, Multiply, Permute, Reshape, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_folder='govdocs1/sample200'\n",
    "minimum=200\n",
    "maximum=200\n",
    "result_dir = 'results/4.3-randomness'\n",
    "os.makedirs(result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawset = Dataset.new_from_folders(raw_dataset_folder).filter_min_max(minimum, maximum)\n",
    "rawset = rawset.filter(lambda x: rawset.category_from(x) not in ['text', 'unk'])\n",
    "rawset.rebuild_categories()\n",
    "rawtset, rawvset = rawset.rnd_split_fraction_by_category(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CM(classes, len_byte_vector):\n",
    "    last = l0 = Input(shape=(512,len_byte_vector))\n",
    "    last = Conv1D(classes, (32,), strides=1)(last)\n",
    "    last = MaxPooling1D(pool_size=481, strides=1)(last)\n",
    "    last = Flatten()(last)\n",
    "    last = Activation('softmax')(last)\n",
    "    name = sys._getframe().f_code.co_name\n",
    "    model = tf.keras.Model([l0], last, name=name)\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatSampler:\n",
    "    def __init__(self, model, blksampler, cat):\n",
    "        self.model = model\n",
    "        self.blksampler = blksampler\n",
    "        self.dataset = blksampler.dataset\n",
    "        self.cat = cat\n",
    "\n",
    "    def __iter__(self):\n",
    "        fails=0\n",
    "        tbenc = iter(BatchEncoder(self.blksampler, 100,\n",
    "                                  xs_encoder='one_hot'))\n",
    "        while True:\n",
    "            blks = next(tbenc)[0]\n",
    "            predict = self.model.predict(blks)\n",
    "            predict = np.argmax(predict, axis=-1)\n",
    "            blks = [x for x,y in zip(blks, predict) if y == self.cat]\n",
    "            if len(blks) == 0:\n",
    "                fails += 1\n",
    "                if fails > 1000:\n",
    "                    raise Exception(\"all blocks seems to be the wrong kind\")\n",
    "                continue\n",
    "            fails=0\n",
    "            for blk in blks:\n",
    "                yield block_sampler.BlockInstance(np.argmax(blk, axis=-1), self.blksampler.dataset.categories[self.cat])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThrSampler:\n",
    "    def __init__(self, model, blksampler, threshold, cat):\n",
    "        self.model = model\n",
    "        self.blksampler = blksampler\n",
    "        self.dataset = blksampler.dataset\n",
    "        self.threshold = threshold\n",
    "        self.cat = cat\n",
    "\n",
    "    def __iter__(self):\n",
    "        fails=0\n",
    "        tbenc = iter(BatchEncoder(self.blksampler, 100,\n",
    "                                  xs_encoder='one_hot'))\n",
    "\n",
    "        while True:\n",
    "            blks = next(tbenc)[0]\n",
    "            predict = self.model.predict(blks, use_multiprocessing=False, workers=0)\n",
    "            predict = predict[:,0]-predict[:,1]\n",
    "            blks = [x for x,y in zip(blks, predict) if y > self.threshold]\n",
    "            if len(blks) == 0:\n",
    "                fails += 1\n",
    "                if fails > 1000:\n",
    "                    raise Exception(\"all blocks seems to be the wrong kind\")\n",
    "                continue\n",
    "            fails=0\n",
    "            for blk in blks:\n",
    "                yield block_sampler.BlockInstance(np.argmax(blk, axis=-1), self.cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=28\n",
    "patience=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "from block_sampler import BlockSamplerByCategory\n",
    "from batch_encoder import BatchEncoder\n",
    "from collections import namedtuple\n",
    "import callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback\n",
    "\n",
    "TrainResults = namedtuple(\n",
    "    'TrainResults', ['model', 'history', 'metrics', 'elapsed'])\n",
    "\n",
    "class RandomTrainer:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 xs_encoder='one_hot',\n",
    "                 validation_steps=steps_per_epoch,\n",
    "                 steps_per_epoch=steps_per_epoch,\n",
    "                 epochs=10000000,\n",
    "                 max_seconds=None,\n",
    "                 batch_size=100,\n",
    "                 min_delta=1e-03,\n",
    "                 patience=patience):\n",
    "        self.model = model\n",
    "        self.xs_encoder = xs_encoder\n",
    "        self.validation_steps = validation_steps\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.max_seconds = max_seconds\n",
    "        self.batch_size = batch_size\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "\n",
    "    def train(self, tset, vset, previous_model=None, previous_model2=None):\n",
    "        tsampler = BlockSamplerByFile(tset)\n",
    "        if previous_model:\n",
    "            tsampler = CatSampler(previous_model, tsampler, 0)\n",
    "        if previous_model2:\n",
    "            tsampler = CatSampler(previous_model2, tsampler, 0)\n",
    "        trsampler = RandomSampler(tsampler, rnd_cat='zzz', not_rnd_cat=tset.categories[0])\n",
    "        tbenc = BatchEncoder(trsampler, self.batch_size,\n",
    "                             xs_encoder=self.xs_encoder)\n",
    "\n",
    "        vsampler = BlockSamplerByFile(vset)\n",
    "        if previous_model:\n",
    "            vsampler = CatSampler(previous_model, vsampler, 0)\n",
    "        if previous_model2:\n",
    "            vsampler = CatSampler(previous_model2, vsampler, 0)\n",
    "        vrsampler = RandomSampler(vsampler, rnd_cat='zzz', not_rnd_cat=vset.categories[0])\n",
    "        vbenc = BatchEncoder(vrsampler, self.batch_size,\n",
    "                             xs_encoder=self.xs_encoder)\n",
    "\n",
    "        return self._train(tbenc,vbenc)\n",
    "\n",
    "    def _train(self, tbenc, vbenc):\n",
    "        model = self.model\n",
    "\n",
    "        timeIt = callbacks.TimeIt()\n",
    "\n",
    "        history = model.fit_generator(iter(tbenc),\n",
    "                                      validation_data=iter(vbenc),\n",
    "                                      validation_steps=self.validation_steps,\n",
    "                                      steps_per_epoch=self.steps_per_epoch,\n",
    "                                      epochs=self.epochs,\n",
    "                                      verbose=0,\n",
    "                                      callbacks=[\n",
    "            timeIt,\n",
    "            # callbacks.SaveModel(os.path.join(result_dir, model.name + '.h5')),\n",
    "#             callbacks.TimeLimit(self.max_seconds),\n",
    "            EarlyStopping(monitor='val_categorical_accuracy',\n",
    "                          min_delta=self.min_delta, patience=self.patience),\n",
    "            # TensorBoard(\n",
    "            #     log_dir=os.path.join(log_dir, model.name),\n",
    "            #     # update_freq=3100,\n",
    "            # ),\n",
    "#             LambdaCallback(on_epoch_end = lambda epoch, logs: print(logs['val_categorical_accuracy'])),\n",
    "        ],\n",
    "            use_multiprocessing=False,\n",
    "            workers=0,\n",
    "        )\n",
    "        return TrainResults(\n",
    "            model=model,\n",
    "            history=history,\n",
    "            metrics=['val_binary_accuracy', 'val_categorical_accuracy'],\n",
    "            elapsed=timeIt.elapsed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndgen(block_instance=False):\n",
    "    while True:\n",
    "        rdata = np.random.randint(0, 256, (100,512), dtype='int')\n",
    "        rdata = one_hot(rdata, 256)\n",
    "        if block_instance:\n",
    "            yield (rdata, np.array([0,1]*100).reshape(100,2))\n",
    "        else:\n",
    "            yield rdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pass1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/atila.alr/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "pptx\n",
      "val_acc = 0.7342857 struct_acc= 0.503 rnd_acc= 0.93\n",
      "ppt\n",
      "val_acc = 0.76785713 struct_acc= 0.583 rnd_acc= 0.977\n",
      "f\n",
      "val_acc = 0.995 struct_acc= 0.99 rnd_acc= 1.0\n",
      "java\n"
     ]
    }
   ],
   "source": [
    "r = Reporter2(result_dir + \"/pass1.tsv\")\n",
    "by_categoryT = rawtset.by_category()\n",
    "by_categoryV = rawvset.by_category()\n",
    "for cat in by_categoryT.keys():\n",
    "    tset = by_categoryT[cat]\n",
    "    vset = by_categoryV[cat]\n",
    "    tset.rebuild_categories(categories=[cat, 'zzz'])\n",
    "    vset.rebuild_categories(categories=[cat, 'zzz'])\n",
    "    model = CM(2, 256)\n",
    "    print(cat)\n",
    "    result = RandomTrainer(\n",
    "        model,\n",
    "        batch_size=100,\n",
    "        steps_per_epoch=28,\n",
    "        validation_steps=28,\n",
    "        patience=10,\n",
    "    ).train(tset, vset)\n",
    "    val_acc = result.history.history['val_categorical_accuracy'][-1]\n",
    "    \n",
    "    vsampler = BlockSamplerByFile(vset)\n",
    "    vbenc = BatchEncoder(vsampler, 100,xs_encoder='one_hot')\n",
    "    struct_acc = model.evaluate_generator(iter(vbenc), steps=10)[-1]\n",
    "    \n",
    "    rnd_acc = model.evaluate_generator(rndgen(True), steps=10)[-1]\n",
    "    \n",
    "    print('val_acc =', val_acc, 'struct_acc=', struct_acc, 'rnd_acc=', rnd_acc)\n",
    "    r.line(category=cat,\n",
    "           val_acc=val_acc,\n",
    "           struct_acc=struct_acc,\n",
    "           rnd_acc=rnd_acc,\n",
    "           **report.report_elapsed(**result._asdict()),\n",
    "           **report.report_epochs(**result._asdict()),\n",
    "          )\n",
    "    h5_path = os.path.join(result_dir, '%s_pass1.h5' % cat)\n",
    "    tf.keras.Model.save(model, h5_path)\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(result_dir + '/pass1.tsv', sep='\\t')\n",
    "data['struct_TP'] = data['struct_acc'] - ((1 - data['struct_acc'])*(1 - data['rnd_acc'])/data['rnd_acc'])\n",
    "data['struct_precision'] = data['struct_TP']/data['struct_acc']\n",
    "data = data.sort_values('struct_TP')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.bar('category', ['struct_TP', 'struct_acc', 'val_acc'], figsize=(12,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.sum(data['struct_TP'])/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(np.sum(data['struct_TP'])+(1-0.22))/28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use models to evaluate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(result_dir + \"/evaluate.tsv\"):\n",
    "    os.remove(result_dir + \"/evaluate.tsv\")\n",
    "r = Reporter2(result_dir + \"/evaluate.tsv\")\n",
    "for cat, vset in by_categoryV.items():\n",
    "    model = tf.keras.models.load_model(result_dir + '/%s_pass1.h5' % cat)\n",
    "\n",
    "    vsampler = BlockSamplerByFile(vset)\n",
    "    vbenc = BatchEncoder(vsampler, 100,xs_encoder='one_hot')\n",
    "    struct_acc = model.evaluate_generator(iter(vbenc), steps=10)[-1]\n",
    "    \n",
    "    rnd_acc = model.evaluate_generator(rndgen(True), steps=10)[-1]\n",
    "    \n",
    "    struct_TP = struct_acc - (1-struct_acc)*(1-rnd_acc)/rnd_acc\n",
    "    \n",
    "    print(cat, struct_acc, rnd_acc, struct_TP )\n",
    "    r.line(cat=cat, struct_acc=struct_acc, rnd_acc=rnd_acc, struct_TP=struct_TP)\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv(result_dir + \"/evaluate.tsv\", sep='\\t')\n",
    "data3 = data3.sort_values('struct_TP')\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "# plt.grid(linestyle='--', which='minor')\n",
    "ax.bar(data3['cat'], data3['struct_TP'], fill=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.tick_params(labelsize=14)\n",
    "for i in range(28):\n",
    "    v = data3.iloc[i,3]\n",
    "    t = ax.annotate('%0.2f'%v, (i-0.3, v-0.06),fontsize=14)\n",
    "    t.set_rotation(90)\n",
    "# data3.plot.bar('cat', ['not_random'], figsize=(12,4))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.sum(data3['struct_TP'])/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(np.sum(data3['struct_TP'])+(1-0.27))/28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train pass2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(result_dir + \"/pass2.tsv\"):\n",
    "    os.remove(result_dir + \"/pass2.tsv\")\n",
    "r = Reporter2(result_dir + \"/pass2.tsv\")\n",
    "by_categoryT = rawtset.by_category()\n",
    "by_categoryV = rawvset.by_category()\n",
    "threshold=0.5\n",
    "for cat in by_categoryT.keys():\n",
    "    tset = by_categoryT[cat]\n",
    "    vset = by_categoryV[cat]\n",
    "    tset.rebuild_categories(categories=[cat, 'zzz'])\n",
    "    vset.rebuild_categories(categories=[cat, 'zzz'])\n",
    "    if data.set_index('category').loc[cat].val_acc > 0.98:\n",
    "        continue\n",
    "    previous_model = tf.keras.models.load_model(result_dir + '/%s_pass1.h5' % cat)\n",
    "    model = CM(2, 256)\n",
    "    print(cat)\n",
    "    batch_size = 100\n",
    "    tsampler = BlockSamplerByFile(tset)\n",
    "    tsampler = ThrSampler(previous_model, tsampler, threshold, cat)\n",
    "    tsampler = RandomSampler(tsampler, rnd_cat='zzz', not_rnd_cat=tset.categories[0])\n",
    "    tbenc = BatchEncoder(tsampler, batch_size,\n",
    "                         xs_encoder='one_hot')\n",
    "\n",
    "    vsampler = BlockSamplerByFile(vset)\n",
    "    vsampler = ThrSampler(previous_model, vsampler, threshold, cat)\n",
    "    vsampler = RandomSampler(vsampler, rnd_cat='zzz', not_rnd_cat=vset.categories[0])\n",
    "    vbenc = BatchEncoder(vsampler, batch_size,\n",
    "                         xs_encoder='one_hot')\n",
    "\n",
    "    result = RandomTrainer(\n",
    "        model,\n",
    "        batch_size=100,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_steps=steps_per_epoch,\n",
    "        patience=patience,\n",
    "    )._train(tbenc,vbenc)\n",
    "    val_acc = result.history.history['val_categorical_accuracy'][-1]\n",
    "\n",
    "    vsampler = BlockSamplerByFile(vset)\n",
    "    vsampler = ThrSampler(previous_model, vsampler, threshold, cat)\n",
    "    vbenc = BatchEncoder(vsampler, 100,xs_encoder='one_hot')\n",
    "    struct_acc = model.evaluate_generator(iter(vbenc), steps=10,use_multiprocessing=False,workers=0)[-1]\n",
    "    \n",
    "    rnd_acc = model.evaluate_generator(rndgen(True), steps=10,use_multiprocessing=False,workers=0)[-1]\n",
    "    \n",
    "    print('val_acc =', val_acc, 'struct_acc=', struct_acc, 'rnd_acc=', rnd_acc)\n",
    "    r.line(category=cat,\n",
    "           val_acc=val_acc,\n",
    "           struct_acc=struct_acc,\n",
    "           rnd_acc=rnd_acc,\n",
    "           **report.report_elapsed(**result._asdict()),\n",
    "           **report.report_epochs(**result._asdict()),\n",
    "          )\n",
    "    h5_path = os.path.join(result_dir, '%s_pass2.h5' % cat)\n",
    "    tf.keras.Model.save(model, h5_path)\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pass2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(result_dir + \"/pass2.tsv\", sep='\\t')\n",
    "data2['struc_precision'] = data2['struct_acc'] - ((1 - data2['struct_acc'])*(1 - data2['rnd_acc'])/data2['rnd_acc'])\n",
    "data2 = data2.sort_values('struc_precision')\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.plot.bar('category', ['struc_precision'], figsize=(12,4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use models to evaluate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(result_dir + \"/evaluate.tsv\"):\n",
    "    os.remove(result_dir + \"/evaluate.tsv\")\n",
    "r = Reporter2(result_dir + \"/evaluate.tsv\")\n",
    "for cat, vset in by_categoryV.items():\n",
    "    model = tf.keras.models.load_model(result_dir + '/%s_pass1.h5' % cat)\n",
    "\n",
    "    vsampler = BlockSamplerByFile(vset)\n",
    "    vbenc = BatchEncoder(vsampler, 100,xs_encoder='one_hot')\n",
    "    struct_acc = model.evaluate_generator(iter(vbenc), steps=10)[-1]\n",
    "    \n",
    "    rnd_acc = model.evaluate_generator(rndgen(True), steps=10)[-1]\n",
    "    \n",
    "    struct_TP = struct_acc - (1-struct_acc)*(1-rnd_acc)/rnd_acc\n",
    "    \n",
    "    print(cat, struct_acc, rnd_acc, struct_TP )\n",
    "    r.line(cat=cat, struct_acc=struct_acc, rnd_acc=rnd_acc, struct_TP=struct_TP)\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.read_csv(result_dir + \"/evaluate.tsv\", sep='\\t')\n",
    "data3 = data3.sort_values('struct_TP')\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "# plt.grid(linestyle='--', which='minor')\n",
    "ax.bar(data3['cat'], data3['struct_TP'], fill=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.tick_params(labelsize=14)\n",
    "for i in range(28):\n",
    "    v = data3.iloc[i,3]\n",
    "    t = ax.annotate('%0.2f'%v, (i-0.3, v-0.06),fontsize=14)\n",
    "    t.set_rotation(90)\n",
    "# data3.plot.bar('cat', ['not_random'], figsize=(12,4))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.sum(data3['struct_TP'])/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(np.sum(data3['struct_TP'])+(1-0.27))/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "# plt.grid(linestyle='--', which='minor')\n",
    "ax.bar(data['category'], data['struct_TP'], fill=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.tick_params(labelsize=14)\n",
    "for i in range(28):\n",
    "    cat = rawvset.ix_to_cat[i]\n",
    "    v = data.iloc[i,6]\n",
    "    t = ax.annotate('%0.2f'%v, (i-0.3, v-0.06),fontsize=14)\n",
    "    t.set_rotation(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roc pass1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_category = rawset.by_category()\n",
    "cat = 'jpg'\n",
    "dataset = by_category[cat]\n",
    "dataset.rebuild_categories(categories=['jpg', 'zzz'])\n",
    "\n",
    "model = tf.keras.models.load_model(result_dir + '/%s_pass1.h5' % cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsampler = BlockSamplerByFile(dataset)\n",
    "tsampler = RandomSampler(tsampler, rnd_cat='zzz', not_rnd_cat='jpg')\n",
    "\n",
    "tbenc = BatchEncoder(tsampler,\n",
    "                     batch_size=1000,\n",
    "                     xs_encoder='one_hot')\n",
    "xs, ys = next(iter(tbenc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalen = len(xs)\n",
    "predict = model.predict(xs, batch_size=100)\n",
    "# predict = np.argmax(predict, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = predict[:,0]-predict[:,1]\n",
    "mask = score > 0.9\n",
    "print(len(score[mask]), len(score))\n",
    "fpr, tpr, thresholds = roc_curve(ys[:,0][mask], score[mask])\n",
    "mask = thresholds > 0\n",
    "fpr[mask], tpr[mask], thresholds[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(predict[:,0]-predict[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use models to evaluate dataset - using pass2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndgen():\n",
    "    while True:\n",
    "        rdata = np.random.randint(0, 256, (100,512), dtype='int')\n",
    "        rdata = one_hot(rdata, 256)\n",
    "        yield rdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(result_dir + \"/evaluate2.tsv\"):\n",
    "    os.remove(result_dir + \"/evaluate2.tsv\")\n",
    "r = Reporter2(result_dir + \"/evaluate2.tsv\")\n",
    "by_category = rawset.by_category()\n",
    "for cat, dataset in by_category.items():\n",
    "    model1 = tf.keras.models.load_model(result_dir + '/%s_pass1.h5' % cat)\n",
    "    tsampler = BlockSamplerByFile(dataset)\n",
    "    tbenc = BatchEncoder(tsampler,\n",
    "                         batch_size=1000,\n",
    "                         xs_encoder='one_hot')\n",
    "    xs, _ = next(iter(tbenc))\n",
    "    datalen = len(xs)\n",
    "    predict = model1.predict(xs, batch_size=100)\n",
    "    predict = np.argmax(predict, axis=-1)\n",
    "\n",
    "    \n",
    "    if os.path.exists(result_dir + '/%s_pass2.h5' % cat):\n",
    "        model2 = tf.keras.models.load_model(result_dir + '/%s_pass2.h5' % cat)\n",
    "        xs = xs[predict==0]\n",
    "        datalen = len(xs)\n",
    "        predict = model2.predict(xs, batch_size=100)\n",
    "        predict = np.argmax(predict, axis=-1)\n",
    "\n",
    "    not_random = len(predict[predict==0])/datalen\n",
    "    \n",
    "    if os.path.exists(result_dir + '/%s_pass2.h5' % cat):\n",
    "        predict = model2.predict_generator(rndgen(), steps=10)\n",
    "    else:\n",
    "        predict = model1.predict_generator(rndgen(), steps=10)\n",
    "    predict = np.argmax(predict, axis=-1)   \n",
    "    random = len(predict[predict==1])/(10*100)\n",
    "    \n",
    "    true_not_random = not_random - (1-not_random)*(1-random)/random\n",
    "    \n",
    "    print(cat, not_random, random, true_not_random )\n",
    "    r.line(cat=cat, not_random=not_random, random=random, true_not_random=true_not_random)\n",
    "r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load evaluation data 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.read_csv(result_dir + \"/evaluate2.tsv\", sep='\\t')\n",
    "data4 = data4.sort_values('true_not_random')\n",
    "data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "ax.yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "# plt.grid(linestyle='--', which='minor')\n",
    "ax.bar(data4['cat'], data4['true_not_random'], fill=False)\n",
    "plt.xticks(rotation='vertical')\n",
    "ax.tick_params(labelsize=14)\n",
    "for i in range(28):\n",
    "    cat = rawset.ix_to_cat[i]\n",
    "    v = data4.iloc[i,3]\n",
    "    t = ax.annotate('%0.2f'%v, (i-0.3, v-0.06),fontsize=14)\n",
    "    t.set_rotation(90)\n",
    "# data3.plot.bar('cat', ['not_random'], figsize=(12,4))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.sum(data4['true_not_random'])/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(np.sum(data4['true_not_random'])+(1-0.62))/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
